<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Attention, learn to solve routing problems | duoluoluo</title><meta name="author" content="duoluoluo"><meta name="copyright" content="duoluoluo"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="旅行商问题（TSP）给定一系列城市和每对城市之间的距离，求解访问每一座城市一次并回到起始城市的最短回路。 这是一个 NP 完全问题，当数据规模比较大的时候，目前几乎不可能找到精确解。 所以本篇文章使用基于 Attention 的模型，同时用一个简单但是有效的 greedy rollout baseline 强化学习方式来训练模型。 当然这个模型不止可以用来处理 TSP 问题，但是文章主要是以 TS">
<meta property="og:type" content="article">
<meta property="og:title" content="Attention, learn to solve routing problems">
<meta property="og:url" content="https://bduoluoluo.github.io/Attention,%20learn%20to%20solve%20routing%20problems/index.html">
<meta property="og:site_name" content="duoluoluo">
<meta property="og:description" content="旅行商问题（TSP）给定一系列城市和每对城市之间的距离，求解访问每一座城市一次并回到起始城市的最短回路。 这是一个 NP 完全问题，当数据规模比较大的时候，目前几乎不可能找到精确解。 所以本篇文章使用基于 Attention 的模型，同时用一个简单但是有效的 greedy rollout baseline 强化学习方式来训练模型。 当然这个模型不止可以用来处理 TSP 问题，但是文章主要是以 TS">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://bduoluoluo.github.io/Attention,%20learn%20to%20solve%20routing%20problems/a.jpg">
<meta property="article:published_time" content="2024-03-09T14:39:27.000Z">
<meta property="article:modified_time" content="2024-03-10T05:39:41.370Z">
<meta property="article:author" content="duoluoluo">
<meta property="article:tag" content="ML4CO">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://bduoluoluo.github.io/Attention,%20learn%20to%20solve%20routing%20problems/a.jpg"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="https://bduoluoluo.github.io/Attention,%20learn%20to%20solve%20routing%20problems/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Attention, learn to solve routing problems',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-03-10 13:39:41'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/style.css"><meta name="generator" content="Hexo 7.0.0"></head><body><script>window.paceOptions = {
  restartOnPushState: false
}

document.addEventListener('pjax:send', () => {
  Pace.restart()
})
</script><link rel="stylesheet" href="/css/corner-indicator.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">5</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background: transparent"><nav id="nav"><span id="blog-info"><a href="/" title="duoluoluo"><img class="site-icon" src="/img/head.jpg"/><span class="site-name">duoluoluo</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Attention, learn to solve routing problems</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-03-09T14:39:27.000Z" title="发表于 2024-03-09 22:39:27">2024-03-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-03-10T05:39:41.370Z" title="更新于 2024-03-10 13:39:41">2024-03-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Attention, learn to solve routing problems"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="旅行商问题（TSP）"><a href="#旅行商问题（TSP）" class="headerlink" title="旅行商问题（TSP）"></a>旅行商问题（TSP）</h1><p>给定一系列城市和每对城市之间的距离，求解访问每一座城市一次并回到起始城市的最短回路。</p>
<p>这是一个 NP 完全问题，当数据规模比较大的时候，目前几乎不可能找到精确解。</p>
<p>所以本篇文章使用基于 Attention 的模型，同时用一个简单但是有效的 greedy rollout baseline 强化学习方式来训练模型。</p>
<p>当然这个模型不止可以用来处理 TSP 问题，但是文章主要是以 TSP 为例进行说明。</p>
<h1 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h1><p>一个问题实例 $s$ 定义为一个有 $n$ 个点的完全图，$x_i$ 表示节点特征，在 TSP 问题中，它是一个 $2$ 维向量，表示在平面上的一个点。TSP 问题的解定义为 $\pi=(\pi_1,\dots,\pi_n)$，是节点的一个排列。</p>
<p>定义基于实例 $s$ 找到解 $\pi$ 的随机策略为：</p>
<script type="math/tex; mode=display">p_{\theta}(\pi|s)=\prod_{t=1}^{n}{p_{\theta}(\pi_t|s,\pi_{1:t-1})}</script><p>然后基于 Encoder-Decoder 框架，找到解 $\pi$。</p>
<h1 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h1><p><img src="../Attention, learn to solve routing problems/encoder.png" alt="Encoder"></p>
<p>输入 $x_i$（batchsize, graphsize, nodedim）。</p>
<p>Embedding $h_i^{(0)}=W^x x_i+b^x$（batchsize, graphsize, embeddim）。</p>
<p>N 层注意力层，每层由两个子层（MHA 和 FF）构成：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{h}_i&=\text{BN}^{\ell}(h_i^{(\ell-1)}+\text{MHA}_i^{\ell}(h_1^{(\ell-1)},\dots,h_n^{(\ell-1)}))\\
h_i^{(\ell)}&=\text{BN}^{\ell}(\hat{h}_i+\text{FF}^{\ell}(\hat{h}_i))\\
\end{aligned}</script><p>在计算最后一层 $h_i^{(N)}$ 时，同时计算均值 $\bar{h}^{(N)}=\frac{1}{n}\sum_{i=1}^{n}{h_i^{(N)}}$，两者都作为 Decoder 的输入。</p>
<h1 id="Attention-mechanism"><a href="#Attention-mechanism" class="headerlink" title="Attention mechanism"></a>Attention mechanism</h1><p><img src="../Attention, learn to solve routing problems/am.png" alt="Attention mechanism"></p>
<p>首先定义键维度 $d_k$ 和值维度 $d_v$。</p>
<p>通过如下计算得到键 $k_i\in\mathbb{R}^{d_k}$，值 $v_i\in\mathbb{R}^{d_v}$ 和询问 $q_i\in\mathbb{R}^{d_k}$：</p>
<script type="math/tex; mode=display">q_i=W^Q h_i,\ k_i=W^K h_i,\ v_i=W^V h_i</script><p>然后计算 compatibility $u_{ij}\in\mathbb{R}$：</p>
<script type="math/tex; mode=display">
u_{ij}=
\left\{\begin{matrix}
\frac{q_i^T k_j}{\sqrt{d_k}} & \text{if}\ i\ \text{adjacent to}\ j \\
-\infty & \text{otherwise}
\end{matrix}\right.</script><p>将其进行 softmax 操作得到注意力权重 $a_{ij}\in[0,1]$：</p>
<script type="math/tex; mode=display">a_{ij}=\frac{e^{u_{ij}}}{\sum_{j'}{e^{u_{ij'}}}}</script><p>最后把权重 $a_{ij}$ 和值 $v_j$ 进行加权求和得到输出：</p>
<script type="math/tex; mode=display">h'_i=\sum_j{a_{ij} v_j}</script><h1 id="Multi-head-attention-MHA"><a href="#Multi-head-attention-MHA" class="headerlink" title="Multi-head attention (MHA)"></a>Multi-head attention (MHA)</h1><p>简单来说就是分别做 M 次注意力机制，再把结果综合起来。</p>
<p>首先定义 $d_k=d_v=\frac{d_h}{M}=16$（模型中的 $d_h$ 和 M 分别为 $128$ 和 $8$），然后得到 M 个结果 $h’_{im}$。</p>
<p>最后利用 M 个可学习的参数矩阵 $W_m^O$ 将结果从 $d_v$ 维映射成 $d_h$ 维：</p>
<script type="math/tex; mode=display">\text{MHA}_i(h_1,\dots,h_n)=\sum_{m=1}^M {W_m^O h'_{im}}</script><h1 id="Feed-forward-sublayer-FF"><a href="#Feed-forward-sublayer-FF" class="headerlink" title="Feed-forward sublayer (FF)"></a>Feed-forward sublayer (FF)</h1><p>前馈子层用到了维度为 $d_{ff}=512$ 的隐藏层和 ReLu 激活函数：</p>
<script type="math/tex; mode=display">\text{FF}(\hat{h}_i)=W^{ff,1}\cdot\text{ReLu}(W^{ff,0}\hat{h}_i+b^{ff,0})+b^{ff,1}</script><h1 id="Batch-normalization-BN"><a href="#Batch-normalization-BN" class="headerlink" title="Batch normalization (BN)"></a>Batch normalization (BN)</h1><p>使用可学习的 $d_h$ 维参数 $w^{bn}$ 和 $b^{bn}$ 进行批量规范化：</p>
<script type="math/tex; mode=display">\text{BN}(h_i)=w^{bn}\odot\overline{\text{BN}}(h_i)+b^{bn}</script><p>$\overline{\text{BN}}$ 代表无仿射变换的批量规范化。</p>
<h1 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h1><p><img src="../Attention, learn to solve routing problems/decoder.png" alt="Attention mechanism"></p>
<p>在时间步 $t\in\{1,\dots,n\}$ 过程中，根据在时间步 $t’&lt;t$ 的结果 $\pi_{t’}$ 来生成本次的结果 $\pi_t$。</p>
<p>在这一过程中我们维护一个 context embedding $h_{(c)}^{(N)}$：</p>
<script type="math/tex; mode=display">
h_{(c)}^{(N)}=
\left\{\begin{matrix}
\left[\bar{h}^{(N)},h_{\pi_{t-1}}^{(N)},h_{\pi_{1}}^{(N)}\right] & t>1 \\
\left[\bar{h}^{(N)},v^1,v^f\right] & t=1
\end{matrix}\right.</script><p>其中 $v^1,v^f$ 分别是两个可学习 $d_h$ 维参数。</p>
<p>而 $h_{\pi_{t-1}}^{(N)},h_{\pi_{1}}^{(N)}$ 分别是已经选取的节点中，最后一个和第一个节点的 embedding，$\bar{h}^{(N)}$ 则是图 embedding。</p>
<p>然后 $[\cdot,\cdot,\cdot]$ 表示水平连接操作，这样我们就得到一个 $(3\cdot d_h)$ 维的 context embedding。</p>
<p>随后加入一个 MHA 层：</p>
<script type="math/tex; mode=display">q_{(c)}=W^Q h_{(c)},\ k_i=W^K h_i,\ v_i=W^V h_i</script><script type="math/tex; mode=display">
u_{(c)j}=
\left\{\begin{matrix}
\frac{q_{(c)}^T k_j}{\sqrt{d_k}} & \text{if}\ j\ne\pi_{t'}\ \forall t'<t \\
-\infty & \text{otherwise}
\end{matrix}\right.</script><p>这样我们可以得到一个新的 context embedding $h_{(c)}^{(N+1)}$，它的维度是 $d_h$ 维的。</p>
<p>最后再加入一个一头注意力层（M=1）：</p>
<script type="math/tex; mode=display">
u_{(c)j}=
\left\{\begin{matrix}
C\cdot\tanh\left(\frac{q_{(c)}^T k_j}{\sqrt{d_k}}\right) & \text{if}\ j\ne\pi_{t'}\ \forall t'<t \\
-\infty & \text{otherwise}
\end{matrix}\right.</script><p>然后再用 softmax 操作计算这一时间步中每个节点的选择概率：</p>
<script type="math/tex; mode=display">p_i=p_{\theta}(\pi_t=i|s,\pi_{1:t-1})=\frac{e^{u_{(c)i}}}{\sum_j{e^{u_{(c)j}}}}</script><p>这里有个需要注意的地方，就是第二个注意力层的询问 $q_{(c)}$ 在文中并没有直接告诉我们是怎么得到的，而且那个新的 context embedding $h_{(c)}^{(N+1)}$ 也并没有说具体作用是什么。</p>
<p>我一开始以为是下一个时间步的 context embedding，但是显然维度对应不上，而且跟文中给出的 Decoder 架构图例也不符，然后我就去翻了翻他的代码。</p>
<p>代码中的 heads 即为 $h_{(c)}^{(N+1)}$，可以看到它经过一个 self.project_out 操作得到 final_Q，即 $q_{(c)}$。</p>
<p><img src="../Attention, learn to solve routing problems/code_0.png" alt="Attention mechanism"></p>
<p>然后这个 self.project_out 操作实际上就是一个全连接层。</p>
<p><img src="../Attention, learn to solve routing problems/code_1.png" alt="Attention mechanism"></p>
<h1 id="Reinforce-with-greedy-rollout-baseline"><a href="#Reinforce-with-greedy-rollout-baseline" class="headerlink" title="Reinforce with greedy rollout baseline"></a>Reinforce with greedy rollout baseline</h1><p>根据给定的实例 $s$，Attention 模型可以从概率分布 $p_{\theta}(\pi|s)$ 中进行采样得到解 $\pi|s$，然后文章定义损失函数为解的期望值，即：</p>
<script type="math/tex; mode=display">\mathcal{L}(\theta|s)=\mathbb{E}_{p_{\theta}(\pi|s)}[L(\pi)]</script><p>然后可以用梯度下降来优化，对于基线 $b(s)$，其梯度为：</p>
<script type="math/tex; mode=display">\nabla\mathcal{L}(\theta|s)=\mathbb{E}_{p_{\theta}(\pi|s)}[(L(\pi)-b(s))\nabla\log{p_{\theta}(\pi|s)}]</script><p>文章采用 rolling out 算法来得到 $b(s)$，为了减少其方差，对于每个时间步都贪心地选择概率最大的节点来使得结果是确定性的。</p>
<p>在每个 epoch 结束后，才会拿最新策略和基线策略在 $10000$ 独立的实例上比较，且只有在有明显提升的时候才用它来更新基线，这样做的目的是防止模型过拟合。</p>
<p><img src="../Attention, learn to solve routing problems/reinforce.png" alt="Attention mechanism"></p>
<h1 id="与其他模型的比较"><a href="#与其他模型的比较" class="headerlink" title="与其他模型的比较"></a>与其他模型的比较</h1><p><img src="../Attention, learn to solve routing problems/compare.png" alt="Attention mechanism"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://Bduoluoluo.github.io">duoluoluo</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://bduoluoluo.github.io/Attention,%20learn%20to%20solve%20routing%20problems/">https://bduoluoluo.github.io/Attention,%20learn%20to%20solve%20routing%20problems/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://Bduoluoluo.github.io" target="_blank">duoluoluo</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/ML4CO/">ML4CO</a></div><div class="post_share"><div class="social-share" data-image="/../Attention,%20learn%20to%20solve%20routing%20problems/a.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/number-theory-basic/" title="number-theory-basic"><img class="cover" src="/../number-theory-basic/a.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">number-theory-basic</div></div></a></div><div class="next-post pull-right"><a href="/NIPS-2015-pointer-networks-Paper/" title="NIPS-2015-pointer-networks-Paper"><img class="cover" src="/../NIPS-2015-pointer-networks-Paper/a.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">NIPS-2015-pointer-networks-Paper</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/NIPS-2015-pointer-networks-Paper/" title="NIPS-2015-pointer-networks-Paper"><img class="cover" src="/../NIPS-2015-pointer-networks-Paper/a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-27</div><div class="title">NIPS-2015-pointer-networks-Paper</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">duoluoluo</div><div class="author-info__description">qwq</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">5</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Bduoluoluo" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://www.zhihu.com/people/duo-luo-luo-93" target="_blank" title="知乎"><i class="fab fa-zhihu" style="color: #0084ff;"></i></a><a class="social-icon" href="https://codeforces.com/profile/duoluoluo" target="_blank" title="Codeforces"><i class="fas fa-laptop-code"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%97%85%E8%A1%8C%E5%95%86%E9%97%AE%E9%A2%98%EF%BC%88TSP%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">旅行商问题（TSP）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-number">2.</span> <span class="toc-text">模型架构</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Encoder"><span class="toc-number">3.</span> <span class="toc-text">Encoder</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Attention-mechanism"><span class="toc-number">4.</span> <span class="toc-text">Attention mechanism</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Multi-head-attention-MHA"><span class="toc-number">5.</span> <span class="toc-text">Multi-head attention (MHA)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Feed-forward-sublayer-FF"><span class="toc-number">6.</span> <span class="toc-text">Feed-forward sublayer (FF)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Batch-normalization-BN"><span class="toc-number">7.</span> <span class="toc-text">Batch normalization (BN)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Decoder"><span class="toc-number">8.</span> <span class="toc-text">Decoder</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Reinforce-with-greedy-rollout-baseline"><span class="toc-number">9.</span> <span class="toc-text">Reinforce with greedy rollout baseline</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%8E%E5%85%B6%E4%BB%96%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-number">10.</span> <span class="toc-text">与其他模型的比较</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/test/" title="test">test</a><time datetime="2025-04-03T05:27:06.000Z" title="发表于 2025-04-03 13:27:06">2025-04-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/dynamic-programming/" title="dynamic-programming"><img src="/../dynamic-programming/a.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="dynamic-programming"/></a><div class="content"><a class="title" href="/dynamic-programming/" title="dynamic-programming">dynamic-programming</a><time datetime="2025-01-19T10:29:09.000Z" title="发表于 2025-01-19 18:29:09">2025-01-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/number-theory-basic/" title="number-theory-basic"><img src="/../number-theory-basic/a.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="number-theory-basic"/></a><div class="content"><a class="title" href="/number-theory-basic/" title="number-theory-basic">number-theory-basic</a><time datetime="2025-01-17T10:51:00.000Z" title="发表于 2025-01-17 18:51:00">2025-01-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Attention,%20learn%20to%20solve%20routing%20problems/" title="Attention, learn to solve routing problems"><img src="/../Attention,%20learn%20to%20solve%20routing%20problems/a.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Attention, learn to solve routing problems"/></a><div class="content"><a class="title" href="/Attention,%20learn%20to%20solve%20routing%20problems/" title="Attention, learn to solve routing problems">Attention, learn to solve routing problems</a><time datetime="2024-03-09T14:39:27.000Z" title="发表于 2024-03-09 22:39:27">2024-03-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/NIPS-2015-pointer-networks-Paper/" title="NIPS-2015-pointer-networks-Paper"><img src="/../NIPS-2015-pointer-networks-Paper/a.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="NIPS-2015-pointer-networks-Paper"/></a><div class="content"><a class="title" href="/NIPS-2015-pointer-networks-Paper/" title="NIPS-2015-pointer-networks-Paper">NIPS-2015-pointer-networks-Paper</a><time datetime="2023-11-27T15:25:04.000Z" title="发表于 2023-11-27 23:25:04">2023-11-27</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(()=>{
  const getGiscusTheme = theme => {
    return theme === 'dark' ? 'dark' : 'light'
  }

  const loadGiscus = () => {
    const config = Object.assign({
      src: 'https://giscus.app/client.js',
      'data-repo': 'Bduoluoluo/Bduoluoluo.github.io',
      'data-repo-id': 'R_kgDOKymNJQ',
      'data-category-id': 'DIC_kwDOKymNJc4CbUmo',
      'data-mapping': 'pathname',
      'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
      'data-reactions-enabled': '1',
      crossorigin: 'anonymous',
      async: true
    },null)

    const ele = document.createElement('script')
    for (let key in config) {
      ele.setAttribute(key, config[key])
    }
    document.getElementById('giscus-wrap').appendChild(ele)
  }

  const changeGiscusTheme = theme => {
    const sendMessage = message => {
      const iframe = document.querySelector('iframe.giscus-frame')
      if (!iframe) return
      iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app')
    }

    sendMessage({
      setConfig: {
        theme: getGiscusTheme(theme)
      }
    });
  }

  btf.addGlobalFn('themeChange', changeGiscusTheme, 'giscus')

  if ('Giscus' === 'Giscus' || !false) {
    if (false) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
    else loadGiscus()
  } else {
    window.loadOtherComment= loadGiscus
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>